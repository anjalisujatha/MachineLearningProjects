import math
import numpy as np
import pandas as pd
import plotly.express as px
import pickle


# Load the training and test datasets
train_data = pd.read_csv('/Users/anjali/Documents/MachineLearningProjects/LinearRegression/train.csv')
test_data = pd.read_csv('/Users/anjali/Documents/MachineLearningProjects/LinearRegression/test.csv')

# Remove rows with missing values
train_data = train_data.dropna()
test_data = test_data.dropna()




train_data.head()


# Plotting scatter plot
px.scatter(x=train_data['x'], y=train_data['y'], template = 'seaborn')


# Set training data and target
X_train = train_data['x'].values
Y_train = train_data['y'].values

# Set testing data and target
X_test = test_data['x'].values
Y_test = test_data['y'].values


def standardize_data(X_train, X_test):
    """
    Standardizes the input data using mean and standard deviation.

    Parameters:
        X_train (numpy.ndarray): Training data.
        X_test (numpy.ndarray): Testing data.

    Returns:
        Tuple of standardized training and testing data.
    """
    # Calculate the mean and standard deviation using the training data
    mean = np.mean(X_train, axis=0)
    std = np.std(X_train, axis=0)

    # Standardize the data
    X_train = (X_train - mean)/std
    X_test = (X_test - mean)/std

    return X_train, X_test

X_train, X_test = standardize_data(X_train, X_test)


X_train = np.expand_dims(X_train, axis =-1)
X_test = np.expand_dims(X_test, axis =1)


class LinearRegression:
    def __init__(self, learning_rate=0.01, convergence_tol = 1e-6):
        """
        Initialize the Linear Regression model.
        Parameters:
        learning_rate (float): The step size for gradient descent.
        convergence_tol (float): The tolerance for convergence.
        """
        self.learning_rate = learning_rate
        self.convergence_tol = convergence_tol
        self.W = None
        self.b = None

    def _initialize_parameters(self, n_features):
        """
        Initialize weights and bias to small random values.

        Parameters:
        n_features (int): Number of features in the dataset.
        """
        self.W = np.random.randn(n_features)*0.01
        self.b = 0

    def _forward(self, X):
        """
        Perform the forward pass to calculate predictions.

        Parameters:
        X (np.ndarray): Input features.

        Returns:
        np.ndarray: Predictions.
        """
        return np.dot(X, self.W) + self.b

    def _compute_cost(self, predictions, Y):
        """
        Compute the mean squared error cost.

        Parameters:
        predictions (np.ndarray): Model predictions.
        Y (np.ndarray): Actual target values.

        Returns:
        float: The cost value.
        """
        m = len(Y)
        return np.sum(np.square(predictions - Y))/(2*m) 
        

    def  _compute_gradients(self, predictions, X, Y):
        """
        Compute gradients for weights and bias.

        Parameters:
        predictions (np.ndarray): Model predictions.
        X (np.ndarray): Input features.
        Y (np.ndarray): Actual target values.

        Returns:
        tuple: Gradients for weights and bias.
        """
        m = len(Y)
        dW = np.dot(X.T, (predictions - Y))/m
        db = np.sum(predictions - Y)/m
        return dW, db

    def fit(self, X, Y, iterations=1000, plot_cost=True):
        """
        Train the model using gradient descent.

        Parameters:
        X (np.ndarray): Input features.
        Y (np.ndarray): Target values.
        iterations (int): Number of training iterations.
        plot_cost (bool): Whether to plot the cost over iterations.
        """
        if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):
            raise TypeError("X and Y must be NumPy arrays.")
            
        if X.shape[0] != Y.shape[0]:
            raise ValueError("X and Y must have the same number of samples.") 

        
        self._initialize_parameters(X.shape[1])
        costs = []
        
        for i in range(iterations):
            predictions = self._forward(X)
            cost = self._compute_cost(predictions, Y)
            dW, db = self._compute_gradients(predictions, X, Y)
            
            # Update parameters
            self.W -= self.learning_rate * dW
            self.b -= self.learning_rate * db
            
            costs.append(cost)
            
            
            if i % 100 == 0:
                print(f"Iteration: {i}, Cost: {cost:.6f}")
                
            # Check for convergence
            if i > 0 and abs(costs[-1] - costs[-2]) < self.convergence_tol:
                print(f"Converged after {i} iterations.")
                break
                
        if plot_cost:
            self._plot_cost(costs)
    def _plot_cost(self, costs):
        """
        Plot the cost over iterations.

        Parameters:
        costs (list): List of cost values.
        """
        fig = px.line(y=costs, title="Cost vs Iteration", template="plotly_dark")
        fig.update_layout(
            title_font_color="#41BEE9",
            xaxis=dict(color="#41BEE9", title="Iterations"),
            yaxis=dict(color="#41BEE9", title="Cost")
        )
        fig.show()

    def predict(self, X):
        """
        Predict target values using the trained model.

        Parameters:
        X (np.ndarray): Input features.

        Returns:
        np.ndarray: Predicted values.
        """
        if self.W is None or self.b is None:
            raise ValueError("Model has not been trained yet. Please call the 'fit' method first.")
        return self._forward(X)

    def save_model(self, filename):
        """
        Save the trained model to a file.

        Parameters:
        filename (str): Path to the file where the model should be saved.
        """
        model_data = {
            'learning_rate': self.learning_rate,
            'convergence_tol': self.convergence_tol,
            'W': self.W,
            'b': self.b
        }
        with open(filename, 'wb') as file:
            pickle.dump(model_data, file)

    @classmethod
    def load_model(cls, filename):
        """
        Load a saved model from a file.

        Parameters:
        filename (str): Path to the file from which to load the model.

        Returns:
        LinearRegression: Loaded model instance.
        """
        with open(filename, 'rb') as file:
            model_data = pickle.load(file)
        model = cls(model_data['learning_rate'], model_data['convergence_tol'])
        model.W = model_data['W']
        model.b = model_data['b']

        return model
       


lr = LinearRegression(learning_rate=0.01)
lr.fit(X_train, Y_train, iterations=10000)


lr.save_model('model.pkl')


model = LinearRegression.load_model("model.pkl")


class RegressionMetrics:
    """
    A utility class for calculating regression evaluation metrics.
    """
    @staticmethod
    def mean_squared_error(Y_true, Y_pred):
        """
        Compute the Mean Squared Error (MSE).

        Parameters:
        Y_true (np.ndarray): True target values.
        Y_pred (np.ndarray): Predicted target values.

        Returns:
        float: The Mean Squared Error.
        """
        assert len(Y_true) == len(Y_pred)
        mse = np.mean((Y_true - Y_pred)**2)
        return mse

    @staticmethod
    def root_mean_squared_error(Y_true, Y_pred):
        """
        Compute the Root Mean Squared Error (RMSE).

        Parameters:
        Y_true (np.ndarray): True target values.
        Y_pred (np.ndarray): Predicted target values.

        Returns:
        float: The Root Mean Squared Error.
        """
        assert len(Y_true) == len(Y_pred)
        mse = RegressionMetrics.mean_squared_error(Y_true, Y_pred)
        rmse = np.sqrt(mse)
        return rmse

    @staticmethod
    def r_squared(Y_true, Y_pred):
        """
        Compute the R-squared (coefficient of determination).

        Parameters:
        Y_true (np.ndarray): True target values.
        Y_pred (np.ndarray): Predicted target values.

        Returns:
        float: The R-squared value.
        """

        assert len(Y_true) == len(Y_pred)
        mean_Y = np.mean(Y_true)
        ss_total = np.sum((Y_true - mean_Y)**2)
        ss_residual = np.sum((Y_true - Y_pred)**2)
        r2 = 1 - (ss_residual/ss_total)
        return r2


Y_pred = model.predict(X_test)
mse_value = RegressionMetrics.mean_squared_error(Y_test, Y_pred)
rmse_value = RegressionMetrics.root_mean_squared_error(Y_test, Y_pred)
r_squared_value = RegressionMetrics.r_squared(Y_test, Y_pred)

print(f"Mean Squared Error (MSE): {mse_value}")
print(f"Root Mean Squared Error (RMSE): {rmse_value}")
print(f"R-squared (Coefficient of Determination): {r_squared_value}")



